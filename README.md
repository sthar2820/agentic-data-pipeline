# ğŸ”¬ Agentic Data Analytics Pipeline

An automated, intelligent data analytics pipeline powered by three specialized AI agents that work together to inspect, refine, and generate insights from your data.

[![CI Pipeline](https://github.com/sthar2820/agentic-data-pipeline/workflows/CI%20Pipeline/badge.svg)](https://github.com/sthar2820/agentic-data-pipeline/actions)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“‹ Overview

This pipeline uses a multi-agent architecture to automate the entire data analytics workflow:

1. **ğŸ” Inspector Agent** - Data profiling and quality validation
2. **ğŸ§¹ Refiner Agent** - Data cleaning and transformation
3. **ğŸ“Š Insight Agent** - Exploratory data analysis and visualization
4. **ğŸ¯ Orchestrator** - Coordinates all agents and manages the pipeline flow
5. **ğŸ’» Streamlit UI** - Interactive web interface for easy interaction

## âœ¨ Features

- âœ… **Automated Data Profiling** - Generate comprehensive data reports with ydata-profiling
- âœ… **Data Quality Validation** - Validate data using Great Expectations
- âœ… **Smart Data Cleaning** - Handle missing values, remove duplicates, unify categories
- âœ… **Advanced Transformations** - Fuzzy matching, imputation, normalization
- âœ… **Rich Visualizations** - Distribution plots, correlation heatmaps, categorical analysis
- âœ… **Interactive UI** - User-friendly Streamlit interface
- âœ… **Comprehensive Logging** - Track all operations with detailed metrics
- âœ… **Multiple File Formats** - Support for CSV, Excel, JSON, Parquet
- âœ… **CI/CD Ready** - GitHub Actions workflow included

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Streamlit UI / CLI                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    Orchestrator      â”‚
              â”‚  (Pipeline Manager)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚
         â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Inspector    â”‚ â”‚  Refiner   â”‚ â”‚  Insight   â”‚
â”‚     Agent      â”‚ â”‚   Agent    â”‚ â”‚   Agent    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Profiling    â”‚ â”‚ â€¢ Cleaning â”‚ â”‚ â€¢ EDA      â”‚
â”‚ â€¢ Validation   â”‚ â”‚ â€¢ Transformâ”‚ â”‚ â€¢ Plots    â”‚
â”‚ â€¢ Quality      â”‚ â”‚ â€¢ Normalizeâ”‚ â”‚ â€¢ Stats    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9 or higher
- pip package manager

### Installation

1. Clone the repository:
```bash
git clone https://github.com/sthar2820/agentic-data-pipeline.git
cd agentic-data-pipeline
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Create necessary directories (already included):
```bash
# Directories are already created
data/
  â”œâ”€â”€ raw/          # Place your input data here
  â”œâ”€â”€ processed/    # Processed data will be saved here
  â””â”€â”€ artifacts/    # Analysis artifacts (reports, plots, metrics)
logs/               # Pipeline logs
```

### Usage

#### Option 1: Command Line Interface (CLI)

```bash
# Basic usage
python main.py --input data/raw/your_data.csv

# With output path
python main.py --input data/raw/your_data.csv --output data/processed/output.csv

# With custom configuration
python main.py --input data/raw/your_data.csv --config configs/pipeline_config.yaml

# Verbose mode
python main.py --input data/raw/your_data.csv --verbose
```

#### Option 2: Streamlit Web UI

```bash
streamlit run ui/app.py
```

Then open your browser to `http://localhost:8501`

#### Option 3: Python API

```python
from orchestrator import PipelineOrchestrator

# Initialize orchestrator
orchestrator = PipelineOrchestrator(config_path='configs/pipeline_config.yaml')

# Run pipeline
results = orchestrator.run_pipeline(
    input_data='data/raw/your_data.csv',
    output_path='data/processed/output.csv',
    dataset_name='my_dataset'
)

# Check results
print(f"Status: {results['status']}")
print(f"Duration: {results['metrics']['duration_seconds']}s")
```

## ğŸ”§ Configuration

Edit `configs/pipeline_config.yaml` to customize the pipeline:

```yaml
# Data paths
data_path: data
artifacts_path: data/artifacts
logs_path: logs

# Pipeline stages
pipeline:
  inspector:
    enabled: true
    profile: true
    validate: true
    
  refiner:
    enabled: true
    operations:
      - clean_column_names
      - handle_missing_values
      - remove_duplicates
    missing_strategy: smart
    
  insight:
    enabled: true
    generate_plots: true
```

## ğŸ“¦ Agent Details

### ğŸ” Inspector Agent

**Technologies:**
- `ydata-profiling` - Automated EDA and profiling
- `Great Expectations` - Data validation framework

**Capabilities:**
- Generate comprehensive data profile reports
- Validate data quality with custom expectations
- Detect missing values, duplicates, and anomalies
- Statistical analysis of all columns
- Data type inference and validation

### ğŸ§¹ Refiner Agent

**Technologies:**
- `pandas` - Core data manipulation
- `pyjanitor` - Data cleaning operations
- `rapidfuzz` - Fuzzy string matching
- `scikit-learn` - Imputation and normalization

**Capabilities:**
- Clean and standardize column names
- Handle missing values (multiple strategies)
- Remove duplicate records
- Unify similar categories using fuzzy matching
- Normalize numeric features
- Advanced imputation (KNN, median, mode)

### ğŸ“Š Insight Agent

**Technologies:**
- `matplotlib` - Static visualizations
- `seaborn` - Statistical plots
- `plotly` - Interactive visualizations

**Capabilities:**
- Generate distribution plots
- Create correlation heatmaps
- Analyze categorical variables
- Produce boxplots for outlier detection
- Interactive dashboards
- Summary statistics

## ğŸ§ª Testing

Run the test suite:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=agents --cov=orchestrator --cov-report=html

# Run specific test file
pytest tests/agents/test_inspector.py -v

# Run with verbose output
pytest -v
```

## ğŸ“Š Example Output

After running the pipeline, you'll find:

```
data/artifacts/
â”œâ”€â”€ dataset_profile.html              # Interactive profile report
â”œâ”€â”€ dataset_profile_stats.json        # Profile statistics
â”œâ”€â”€ validation_results.json           # Data quality results
â”œâ”€â”€ transformation_log.json           # Cleaning operations log
â”œâ”€â”€ summary_statistics.json           # Statistical summary
â”œâ”€â”€ distribution_plots.png            # Distribution visualizations
â”œâ”€â”€ correlation_heatmap.png           # Correlation analysis
â”œâ”€â”€ boxplots.png                      # Outlier detection
â””â”€â”€ pipeline_metrics_dataset.json    # Pipeline execution metrics

logs/
â””â”€â”€ pipeline_YYYYMMDD_HHMMSS.log     # Detailed execution logs
```

## ğŸ› ï¸ Development

### Code Quality

```bash
# Format code
black agents/ orchestrator/ ui/ main.py

# Sort imports
isort agents/ orchestrator/ ui/ main.py

# Lint code
flake8 agents/ orchestrator/ ui/

# Type checking
mypy agents/ orchestrator/
```

### Project Structure

```
agentic-data-pipeline/
â”œâ”€â”€ agents/                    # Agent modules
â”‚   â”œâ”€â”€ inspector/            # Inspector agent
â”‚   â”œâ”€â”€ refiner/              # Refiner agent
â”‚   â””â”€â”€ insight/              # Insight agent
â”œâ”€â”€ orchestrator/             # Pipeline orchestrator
â”œâ”€â”€ ui/                       # Streamlit UI
â”œâ”€â”€ configs/                  # Configuration files
â”œâ”€â”€ tests/                    # Test suite
â”‚   â”œâ”€â”€ agents/              # Agent tests
â”‚   â”œâ”€â”€ orchestrator/        # Orchestrator tests
â”‚   â””â”€â”€ conftest.py          # Test fixtures
â”œâ”€â”€ data/                     # Data directories
â”‚   â”œâ”€â”€ raw/                 # Input data
â”‚   â”œâ”€â”€ processed/           # Output data
â”‚   â””â”€â”€ artifacts/           # Analysis artifacts
â”œâ”€â”€ logs/                     # Log files
â”œâ”€â”€ .github/workflows/        # CI/CD workflows
â”œâ”€â”€ main.py                   # CLI entry point
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                # This file
```

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [ydata-profiling](https://github.com/ydataai/ydata-profiling) for automated profiling
- [Great Expectations](https://greatexpectations.io/) for data validation
- [pyjanitor](https://github.com/pyjanitor-devs/pyjanitor) for data cleaning
- [Streamlit](https://streamlit.io/) for the web framework

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

Made with â¤ï¸ by the community
